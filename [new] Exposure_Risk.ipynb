{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import math\n",
    "from collections import defaultdict"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Read the preprocessed dataset:\n",
    " - \"origin_dest_trips_census_tract_level.csv\"\n",
    " - \"tracts_4.csv\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# this dataset is for the nested hashmap\n",
    "OD_p1_df = pd.read_csv('Streetlight_Data/clean_data/streetlight_OD_p1.csv')[['RESIDENCE','WORKPLACE','JOBS']]\n",
    "OD_p2_df = pd.read_csv('Streetlight_Data/clean_data/streetlight_OD_p2.csv')[['RESIDENCE','WORKPLACE','JOBS']]\n",
    "OD_p3_df = pd.read_csv('Streetlight_Data/clean_data/streetlight_OD_p3.csv')[['RESIDENCE','WORKPLACE','JOBS']]\n",
    "OD_p4_df = pd.read_csv('Streetlight_Data/clean_data/streetlight_OD_p4.csv')[['RESIDENCE','WORKPLACE','JOBS']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>RESIDENCE</th>\n",
       "      <th>WORKPLACE</th>\n",
       "      <th>JOBS</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>17001000100</td>\n",
       "      <td>17001000100</td>\n",
       "      <td>5005.177622</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>17001000100</td>\n",
       "      <td>17001000201</td>\n",
       "      <td>432.101665</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>17001000100</td>\n",
       "      <td>17001000202</td>\n",
       "      <td>324.076249</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>17001000100</td>\n",
       "      <td>17001000400</td>\n",
       "      <td>360.084721</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>17001000100</td>\n",
       "      <td>17001000500</td>\n",
       "      <td>4285.008180</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>80423</th>\n",
       "      <td>17203030700</td>\n",
       "      <td>17113005102</td>\n",
       "      <td>46.336570</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>80424</th>\n",
       "      <td>17203030700</td>\n",
       "      <td>17143001600</td>\n",
       "      <td>36.275552</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>80425</th>\n",
       "      <td>17203030700</td>\n",
       "      <td>17179021201</td>\n",
       "      <td>155.851852</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>80426</th>\n",
       "      <td>17203030700</td>\n",
       "      <td>17203030601</td>\n",
       "      <td>480.273810</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>80427</th>\n",
       "      <td>17203030700</td>\n",
       "      <td>17203030700</td>\n",
       "      <td>7924.517857</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>80428 rows × 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "         RESIDENCE    WORKPLACE         JOBS\n",
       "0      17001000100  17001000100  5005.177622\n",
       "1      17001000100  17001000201   432.101665\n",
       "2      17001000100  17001000202   324.076249\n",
       "3      17001000100  17001000400   360.084721\n",
       "4      17001000100  17001000500  4285.008180\n",
       "...            ...          ...          ...\n",
       "80423  17203030700  17113005102    46.336570\n",
       "80424  17203030700  17143001600    36.275552\n",
       "80425  17203030700  17179021201   155.851852\n",
       "80426  17203030700  17203030601   480.273810\n",
       "80427  17203030700  17203030700  7924.517857\n",
       "\n",
       "[80428 rows x 3 columns]"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "OD_p1_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>GEOID</th>\n",
       "      <th>countyFIPS</th>\n",
       "      <th>estimate_cases1</th>\n",
       "      <th>estimate_cases2</th>\n",
       "      <th>estimate_cases3</th>\n",
       "      <th>estimate_cases4</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>17091011700</td>\n",
       "      <td>17091</td>\n",
       "      <td>0.030119</td>\n",
       "      <td>25.450775</td>\n",
       "      <td>36.353948</td>\n",
       "      <td>114.031521</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>17091011800</td>\n",
       "      <td>17091</td>\n",
       "      <td>0.023156</td>\n",
       "      <td>19.566633</td>\n",
       "      <td>27.949026</td>\n",
       "      <td>87.667780</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>17119400951</td>\n",
       "      <td>17119</td>\n",
       "      <td>0.018442</td>\n",
       "      <td>9.442116</td>\n",
       "      <td>15.933571</td>\n",
       "      <td>140.008883</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>17119400952</td>\n",
       "      <td>17119</td>\n",
       "      <td>0.012385</td>\n",
       "      <td>6.341011</td>\n",
       "      <td>10.700455</td>\n",
       "      <td>94.025297</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>17135957500</td>\n",
       "      <td>17135</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>4.240201</td>\n",
       "      <td>4.783816</td>\n",
       "      <td>72.953196</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3118</th>\n",
       "      <td>17037000100</td>\n",
       "      <td>17037</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>25.211487</td>\n",
       "      <td>33.955725</td>\n",
       "      <td>174.629441</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3119</th>\n",
       "      <td>17037001500</td>\n",
       "      <td>17037</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>14.299781</td>\n",
       "      <td>19.259452</td>\n",
       "      <td>99.048612</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3120</th>\n",
       "      <td>17037000400</td>\n",
       "      <td>17037</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>32.385793</td>\n",
       "      <td>43.618334</td>\n",
       "      <td>224.322860</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3121</th>\n",
       "      <td>17037000300</td>\n",
       "      <td>17037</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>10.066565</td>\n",
       "      <td>13.558007</td>\n",
       "      <td>69.726892</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3122</th>\n",
       "      <td>17037000200</td>\n",
       "      <td>17037</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>18.829736</td>\n",
       "      <td>25.360555</td>\n",
       "      <td>130.425713</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>3123 rows × 6 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "            GEOID  countyFIPS  estimate_cases1  estimate_cases2  \\\n",
       "0     17091011700       17091         0.030119        25.450775   \n",
       "1     17091011800       17091         0.023156        19.566633   \n",
       "2     17119400951       17119         0.018442         9.442116   \n",
       "3     17119400952       17119         0.012385         6.341011   \n",
       "4     17135957500       17135         0.000000         4.240201   \n",
       "...           ...         ...              ...              ...   \n",
       "3118  17037000100       17037         0.000000        25.211487   \n",
       "3119  17037001500       17037         0.000000        14.299781   \n",
       "3120  17037000400       17037         0.000000        32.385793   \n",
       "3121  17037000300       17037         0.000000        10.066565   \n",
       "3122  17037000200       17037         0.000000        18.829736   \n",
       "\n",
       "      estimate_cases3  estimate_cases4  \n",
       "0           36.353948       114.031521  \n",
       "1           27.949026        87.667780  \n",
       "2           15.933571       140.008883  \n",
       "3           10.700455        94.025297  \n",
       "4            4.783816        72.953196  \n",
       "...               ...              ...  \n",
       "3118        33.955725       174.629441  \n",
       "3119        19.259452        99.048612  \n",
       "3120        43.618334       224.322860  \n",
       "3121        13.558007        69.726892  \n",
       "3122        25.360555       130.425713  \n",
       "\n",
       "[3123 rows x 6 columns]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ct_info_cases_data = pd.read_csv(\"Statistical_Analysis/data/ct_info_cases_data.csv\")\n",
    "ct_cases_df = ct_info_cases_data[['GEOID','countyFIPS','estimate_cases1','estimate_cases2','estimate_cases3','estimate_cases4']]\n",
    "ct_cases_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create a dictionary: to record each origin->destination: number of trips\n",
    "\n",
    "use the dataset: OD_census_tract_level "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Generate a unique set of origins in commute trips"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2. Generate a doubly nested hashmap (python dictionary: key-value pair): \\\n",
    " first mapping: **origin -> destination** \\\n",
    " second mapping: **destination -> number of trips**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#function to fo defaultdict(defaultdict(int))\n",
    "# when {} is empty, enable\n",
    "# df['a']['b']+=1\n",
    "def defaultdict_float():\n",
    "    return defaultdict(float)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-5-8717fd0cbe90>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     11\u001b[0m     \u001b[0mfip_d\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mOD\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0miloc\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m     \u001b[0mnum_trips\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mOD\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0miloc\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 13\u001b[0;31m     \u001b[0mnum_cases\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfloat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mct_cases_df\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mct_cases_df\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'GEOID'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0mfip_o\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'estimate_cases1'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     14\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mnum_cases\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     15\u001b[0m         \u001b[0;32mcontinue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/anaconda3/lib/python3.7/site-packages/pandas/core/frame.py\u001b[0m in \u001b[0;36m__getitem__\u001b[0;34m(self, key)\u001b[0m\n\u001b[1;32m   3013\u001b[0m         \u001b[0;31m# Do we have a (boolean) 1d indexer?\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3014\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mcom\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mis_bool_indexer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3015\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_getitem_bool_array\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   3016\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3017\u001b[0m         \u001b[0;31m# We are left with two options: a single key, and a collection of keys,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/anaconda3/lib/python3.7/site-packages/pandas/core/frame.py\u001b[0m in \u001b[0;36m_getitem_bool_array\u001b[0;34m(self, key)\u001b[0m\n\u001b[1;32m   3068\u001b[0m         \u001b[0mkey\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcheck_bool_indexer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mindex\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3069\u001b[0m         \u001b[0mindexer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mkey\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnonzero\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3070\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_take_with_is_copy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mindexer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   3071\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3072\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_getitem_multilevel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/anaconda3/lib/python3.7/site-packages/pandas/core/generic.py\u001b[0m in \u001b[0;36m_take_with_is_copy\u001b[0;34m(self, indices, axis)\u001b[0m\n\u001b[1;32m   3600\u001b[0m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtake\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mindices\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mindices\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0maxis\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3601\u001b[0m         \u001b[0;31m# Maybe set copy if we didn't actually change the index.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3602\u001b[0;31m         \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_get_axis\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0maxis\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mequals\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_get_axis\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0maxis\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   3603\u001b[0m             \u001b[0mresult\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_set_is_copy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3604\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/anaconda3/lib/python3.7/site-packages/pandas/core/indexes/base.py\u001b[0m in \u001b[0;36mequals\u001b[0;34m(self, other)\u001b[0m\n\u001b[1;32m   4456\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   4457\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 4458\u001b[0;31m         \u001b[0;32mif\u001b[0m \u001b[0mis_object_dtype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mis_object_dtype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mother\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   4459\u001b[0m             \u001b[0;31m# if other is not object, use other's logic for coercion\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   4460\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mother\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mequals\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/anaconda3/lib/python3.7/site-packages/pandas/core/dtypes/common.py\u001b[0m in \u001b[0;36mis_object_dtype\u001b[0;34m(arr_or_dtype)\u001b[0m\n\u001b[1;32m    226\u001b[0m     \u001b[0;32mFalse\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    227\u001b[0m     \"\"\"\n\u001b[0;32m--> 228\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0m_is_dtype_type\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0marr_or_dtype\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mclasses\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mobject_\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    229\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    230\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# anuallly aggregated data \n",
    "# origin -> destination (county level) frequency dictionary\n",
    "# rename the origin-> destination df\n",
    "OD = OD_p1_df\n",
    "\n",
    "nu_OD_DblMap = defaultdict(defaultdict_float)\n",
    "num_ct = OD_p1_df.shape[0]\n",
    "\n",
    "for i in range(num_ct):\n",
    "    fip_o = OD.iloc[i,0]\n",
    "    fip_d = OD.iloc[i,1]\n",
    "    num_trips = OD.iloc[i,2]   \n",
    "    num_cases = float(ct_cases_df[ct_cases_df['GEOID'] == fip_o]['estimate_cases1'])\n",
    "    if num_cases == 0:\n",
    "        continue\n",
    "    else:\n",
    "        nu_OD_DblMap[fip_o][fip_d] = num_trips * num_cases"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "de_OD_Map = defaultdict(float)\n",
    "for fip_d in nu_OD_DblMap:\n",
    "    sum = 0\n",
    "    for fip_o in nu_OD_DblMap:\n",
    "        sum += nu_OD_DblMap[fip_o][fip_d]\n",
    "    de_OD_Map[fip_d] = sum  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "Risk_DblMap = defaultdict(defaultdict_float)\n",
    "for fip_o in nu_OD_DblMap:\n",
    "    for fip_d in nu_OD_DblMap[fip_o]:\n",
    "        nu = nu_OD_DblMap[fip_o][fip_d]\n",
    "        de = de_OD_Map[fip_d]\n",
    "        if de == 0 or nu == 0:\n",
    "            continue\n",
    "        else:               \n",
    "            Risk_DblMap[fip_o][fip_d] = nu/de"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# new\n",
    "def getUnnormalizedRisk(OD, estimate_cases, apple_mobility):\n",
    "    nu_OD_DblMap = defaultdict(defaultdict_float)\n",
    "    num_ct = OD.shape[0]\n",
    "\n",
    "    for i in range(num_ct):\n",
    "        fip_o = OD.iloc[i,0]\n",
    "        fip_d = OD.iloc[i,1]\n",
    "        num_trips = OD.iloc[i,2]   \n",
    "        num_cases = float(ct_cases_df[ct_cases_df['GEOID'] == fip_o][estimate_cases])\n",
    "        if num_cases == 0:\n",
    "            continue\n",
    "        else:\n",
    "            nu_OD_DblMap[fip_o][fip_d] = num_trips * num_cases * apple_mobility\n",
    "    return nu_OD_DblMap"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getRisk(OD, estimate_cases, apple_mobility):      \n",
    "    nu_OD_DblMap = defaultdict(defaultdict_float)\n",
    "    num_ct = OD.shape[0]\n",
    "    for i in range(num_ct):\n",
    "        fip_o = OD.iloc[i,0]\n",
    "        fip_d = OD.iloc[i,1]\n",
    "        num_trips = OD.iloc[i,2]   \n",
    "        num_cases = float(ct_cases_df[ct_cases_df['GEOID'] == fip_o][estimate_cases])\n",
    "        if num_cases == 0:\n",
    "            continue\n",
    "        else:\n",
    "            nu_OD_DblMap[fip_o][fip_d] = num_trips * num_cases * apple_mobility\n",
    "\n",
    "    de_OD_Map = defaultdict(float)\n",
    "    for fip_d in nu_OD_DblMap:\n",
    "        sum = 0\n",
    "        for fip_o in nu_OD_DblMap:\n",
    "            sum += nu_OD_DblMap[fip_o][fip_d]\n",
    "        de_OD_Map[fip_d] = sum  \n",
    "\n",
    "    Risk_DblMap = defaultdict(defaultdict_float)\n",
    "    for fip_o in nu_OD_DblMap:\n",
    "        for fip_d in nu_OD_DblMap[fip_o]:\n",
    "            nu = nu_OD_DblMap[fip_o][fip_d]\n",
    "            de = de_OD_Map[fip_d]\n",
    "            if de == 0 or nu == 0:\n",
    "                continue\n",
    "            else:               \n",
    "                Risk_DblMap[fip_o][fip_d] = nu/de\n",
    "    return Risk_DblMap"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getDPTotalRisk(OD, estimate_cases, apple_mobility):\n",
    "    nu_OD_DblMap = defaultdict(defaultdict_float)\n",
    "    num_ct = OD.shape[0]\n",
    "    total_riskflow = 0\n",
    "\n",
    "    for i in range(num_ct):\n",
    "        fip_o = OD.iloc[i,0]\n",
    "        fip_d = OD.iloc[i,1]\n",
    "        num_trips = OD.iloc[i,2]   \n",
    "        num_cases = float(ct_cases_df[ct_cases_df['GEOID'] == fip_o][estimate_cases])\n",
    "        total_riskflow += num_trips * num_cases \n",
    "        if num_cases == 0:\n",
    "            continue\n",
    "        else:\n",
    "            nu_OD_DblMap[fip_o][fip_d] = num_trips * num_cases\n",
    "        \n",
    "    for fip_o in nu_OD_DblMap:\n",
    "        for fip_d in nu_OD_DblMap[fip_o]:\n",
    "            if nu_OD_DblMap[fip_o][fip_d] == 0:\n",
    "                continue\n",
    "            else:\n",
    "                nu_OD_DblMap[fip_o][fip_d] = nu_OD_DblMap[fip_o][fip_d]/total_riskflow * apple_mobility\n",
    "    print(total_riskflow)       \n",
    "    return nu_OD_DblMap"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "unor_Risk_DblMap1 =  getUnnormalizedRisk(OD = OD_p1_df, \n",
    "                       estimate_cases = 'estimate_cases1',\n",
    "                       apple_mobility = 109.278621/100)\n",
    "unor_Risk_DblMap2 =  getUnnormalizedRisk(OD = OD_p2_df, \n",
    "                       estimate_cases = 'estimate_cases2',\n",
    "                       apple_mobility = 73.2133333/100)\n",
    "unor_Risk_DblMap3 =  getUnnormalizedRisk(OD = OD_p3_df, \n",
    "                       estimate_cases = 'estimate_cases3',\n",
    "                       apple_mobility = 130.970323/100)\n",
    "unor_Risk_DblMap4 =  getUnnormalizedRisk(OD = OD_p4_df,\n",
    "                       estimate_cases = 'estimate_cases4',\n",
    "                       apple_mobility = 139.87/100)         "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "list_unor_Risk_DblMap = [unor_Risk_DblMap1, unor_Risk_DblMap2, unor_Risk_DblMap3, unor_Risk_DblMap4]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "Risk_DblMap1 = getRisk(OD = OD_p1_df, \n",
    "                       estimate_cases = 'estimate_cases1',\n",
    "                       apple_mobility = 109.278621/100)\n",
    "Risk_DblMap2 = getRisk(OD = OD_p2_df, \n",
    "                       estimate_cases = 'estimate_cases2',\n",
    "                       apple_mobility = 73.2133333/100)\n",
    "Risk_DblMap3 = getRisk(OD = OD_p3_df, \n",
    "                       estimate_cases = 'estimate_cases3',\n",
    "                       apple_mobility = 130.970323/100)\n",
    "Risk_DblMap4 = getRisk(OD = OD_p4_df,\n",
    "                       estimate_cases = 'estimate_cases4',\n",
    "                       apple_mobility = 139.87/100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "list_Risk_DblMap = [Risk_DblMap1, Risk_DblMap2, Risk_DblMap3, Risk_DblMap4]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8836264.208173046\n",
      "1810276645.1046925\n",
      "2281021844.448017\n",
      "6247458949.558438\n"
     ]
    }
   ],
   "source": [
    "dptotal_Risk_DblMap1 =  getDPTotalRisk(OD = OD_p1_df, \n",
    "                                         estimate_cases = 'estimate_cases1',\n",
    "                                         apple_mobility = 109.278621/100)\n",
    "dptotal_Risk_DblMap2 =  getDPTotalRisk(OD = OD_p2_df,\n",
    "                                         estimate_cases = 'estimate_cases2',\n",
    "                                         apple_mobility = 73.2133333/100)\n",
    "dptotal_Risk_DblMap3 =  getDPTotalRisk(OD = OD_p3_df,\n",
    "                                         estimate_cases = 'estimate_cases3',\n",
    "                                         apple_mobility = 130.970323/100)\n",
    "dptotal_Risk_DblMap4 =  getDPTotalRisk(OD = OD_p4_df,\n",
    "                                         estimate_cases = 'estimate_cases4',\n",
    "                                         apple_mobility = 139.87/100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "list_dptotal_Risk_DblMap = [dptotal_Risk_DblMap1, dptotal_Risk_DblMap2, \n",
    "                             dptotal_Risk_DblMap3, dptotal_Risk_DblMap4]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Transfer double risk map to csv file and save to path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def saveRiskflowToCSV(Risk_DblMap, path):\n",
    "    riskflow_df = pd.DataFrame(columns = ('START', 'END', 'RISK_FLOW'))\n",
    "    for fip_o in Risk_DblMap:\n",
    "        for fip_d in Risk_DblMap[fip_o]:\n",
    "            riskflow = Risk_DblMap[fip_o][fip_d]\n",
    "            new_row = pd.DataFrame({'START':[fip_o], 'END':[fip_d], 'RISK_FLOW': [riskflow] })\n",
    "            riskflow_df = riskflow_df.append(new_row)\n",
    "    riskflow_df.to_csv(path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "saveRiskflowToCSV(Risk_DblMap = unor_Risk_DblMap1, \n",
    "                  path = 'Streetlight_Data/clean_data/unnorm/unor_riskflow1_df.csv')\n",
    "saveRiskflowToCSV(Risk_DblMap = unor_Risk_DblMap2, \n",
    "                  path = 'Streetlight_Data/clean_data/unnorm/unor_riskflow2_df.csv')\n",
    "saveRiskflowToCSV(Risk_DblMap = unor_Risk_DblMap3, \n",
    "                  path = 'Streetlight_Data/clean_data/unnorm/unor_riskflow3_df.csv')\n",
    "saveRiskflowToCSV(Risk_DblMap = unor_Risk_DblMap4, \n",
    "                  path = 'Streetlight_Data/clean_data/unnorm/unor_riskflow4_df.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "saveRiskflowToCSV(Risk_DblMap = Risk_DblMap1, \n",
    "                  path = 'Streetlight_Data/clean_data/riskflow1_df.csv')\n",
    "saveRiskflowToCSV(Risk_DblMap = Risk_DblMap2, \n",
    "                  path = 'Streetlight_Data/clean_data/riskflow2_df.csv')\n",
    "saveRiskflowToCSV(Risk_DblMap = Risk_DblMap3, \n",
    "                  path = 'Streetlight_Data/clean_data/riskflow3_df.csv')\n",
    "saveRiskflowToCSV(Risk_DblMap = Risk_DblMap4, \n",
    "                  path = 'Streetlight_Data/clean_data/riskflow4_df.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "saveRiskflowToCSV(Risk_DblMap = dptotal_Risk_DblMap1, \n",
    "                  path = 'Streetlight_Data/clean_data/dptotal/dptotal_riskflow1_df.csv')\n",
    "saveRiskflowToCSV(Risk_DblMap = dptotal_Risk_DblMap2, \n",
    "                  path = 'Streetlight_Data/clean_data/dptotal/dptotal_riskflow2_df.csv')\n",
    "saveRiskflowToCSV(Risk_DblMap = dptotal_Risk_DblMap3, \n",
    "                  path = 'Streetlight_Data/clean_data/dptotal/dptotal_riskflow3_df.csv')\n",
    "saveRiskflowToCSV(Risk_DblMap = dptotal_Risk_DblMap4, \n",
    "                  path = 'Streetlight_Data/clean_data/dptotal/dptotal_riskflow4_df.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Calculaye exposure entropy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Exposure_Map = defaultdict(defaultdict_float)\n",
    "de_Risk_Map = defaultdict(float)\n",
    "for fip_d in Risk_DblMap:\n",
    "    sum = 0\n",
    "    for fip_o in Risk_DblMap[fip_o]:\n",
    "        sum += Risk_DblMap[fip_o][fip_d]\n",
    "    de_Risk_Map[fip_d] = sum "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "reverse_Risk_DblMap = defaultdict(defaultdict_float)\n",
    "for fip_o in Risk_DblMap:\n",
    "    for fip_d in Risk_DblMap[fip_o]:\n",
    "        risk = Risk_DblMap[fip_o][fip_d]   \n",
    "        if risk == 0:\n",
    "            continue\n",
    "        else:\n",
    "            reverse_Risk_DblMap[fip_d][fip_o] = risk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.sum(list(reverse_Risk_DblMap[17031811701].values()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Exposure_Map = {}\n",
    "for fip_d in reverse_Risk_DblMap:\n",
    "    v = np.array(list(reverse_Risk_DblMap[fip_d].values()))\n",
    "    logv = np.log(v)\n",
    "    entropy = - np.sum(v * logv)\n",
    "    Exposure_Map[fip_d] = entropy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getExposure(Risk_DblMap):\n",
    "#     reverse_Risk_DblMap = defaultdict(defaultdict_float)\n",
    "#     for fip_o in Risk_DblMap:\n",
    "#         for fip_d in Risk_DblMap[fip_o]:\n",
    "#             risk = Risk_DblMap[fip_o][fip_d]   \n",
    "#             if risk == 0:\n",
    "#                 continue\n",
    "#             else:\n",
    "#                 reverse_Risk_DblMap[fip_d][fip_o] = risk\n",
    "\n",
    "    Exposure_Map = {}\n",
    "    for fip_d in Risk_DblMap:\n",
    "        unor_v = np.array(list(Risk_DblMap[fip_d].values()))\n",
    "        Sigma = np.sum(unor_v)\n",
    "        unor_v[unor_v == 0] = 10**-10\n",
    "        v = unor_v/Sigma\n",
    "        v = unor_v\n",
    "        logv = np.log(v)\n",
    "        entropy =  np.sum(v * logv)\n",
    "        Exposure_Map[fip_d] = entropy\n",
    "    return Exposure_Map"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "metadata": {},
   "outputs": [],
   "source": [
    "unor_Exposure_Map4 = getExposure(unor_Risk_DblMap4)\n",
    "unor_Exposure_Map3 = getExposure(unor_Risk_DblMap3)\n",
    "unor_Exposure_Map2 = getExposure(unor_Risk_DblMap2)\n",
    "unor_Exposure_Map1 = getExposure(unor_Risk_DblMap1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [],
   "source": [
    "Risk_DblMap = unor_Risk_DblMap4\n",
    "Exposure_Map = {}\n",
    "for fip_d in Risk_DblMap:\n",
    "    unor_v = np.array(list(Risk_DblMap[fip_d].values()))\n",
    "    Sigma = np.sum(unor_v)\n",
    "    unor_v[unor_v == 0] = 10**-10\n",
    "    v = unor_v/Sigma\n",
    "    logv = np.log(v)\n",
    "    entropy = np.sum(v * logv)\n",
    "    Exposure_Map[fip_d] = entropy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "metadata": {},
   "outputs": [],
   "source": [
    "def saveExposureToCSV(list_Risk_DblMap, path):\n",
    "    Exposure_Map1 = getExposure(list_Risk_DblMap[0])\n",
    "    Exposure_Map2 = getExposure(list_Risk_DblMap[1])\n",
    "    Exposure_Map3 = getExposure(list_Risk_DblMap[2])\n",
    "    Exposure_Map4 = getExposure(list_Risk_DblMap[3])\n",
    "    \n",
    "    exposure_df = pd.DataFrame(columns = ('FIPS', 'exposure1', 'exposure2', 'exposure3', 'exposure4'))\n",
    "    \n",
    "    for i, fip_d in enumerate(Exposure_Map1): \n",
    "        FIPS = str(fip_d)\n",
    "        exposure1 = Exposure_Map1[fip_d]\n",
    "        exposure2 = Exposure_Map2[fip_d]\n",
    "        exposure3 = Exposure_Map3[fip_d]\n",
    "        exposure4 = Exposure_Map4.get(fip_d, 0)\n",
    "        \n",
    "        new_row = pd.DataFrame({'FIPS':[FIPS], 'exposure1':[exposure1],\n",
    "                            'exposure2':[exposure2], 'exposure3':[exposure3],\n",
    "                            'exposure4':[exposure4]})\n",
    "        exposure_df = exposure_df.append(new_row)\n",
    "    \n",
    "    exposure_df.to_csv(path)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "metadata": {},
   "outputs": [],
   "source": [
    "saveExposureToCSV(list_unor_Risk_DblMap, 'Streetlight_Data/clean_data/unnorm/unor_exposure_df.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'list_Risk_DblMap' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-147-730c52bc91d0>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0msaveExposureToCSV\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlist_Risk_DblMap\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'Streetlight_Data/clean_data/risk/unor_exposure_df.csv'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m: name 'list_Risk_DblMap' is not defined"
     ]
    }
   ],
   "source": [
    "saveExposureToCSV(list_Risk_DblMap, 'Streetlight_Data/clean_data/risk/exposure_df.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "saveExposureToCSV(list_dptotal_Risk_DblMap, 'Streetlight_Data/clean_data/dptotal/dptotal_exposure_df.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getInlfowExposure(Risk_DblMap):\n",
    "    reverse_Risk_DblMap = defaultdict(defaultdict_float)\n",
    "    for fip_o in Risk_DblMap:\n",
    "        for fip_d in Risk_DblMap[fip_o]:\n",
    "            risk = Risk_DblMap[fip_o][fip_d]   \n",
    "            if risk == 0:\n",
    "                continue\n",
    "            else:\n",
    "                reverse_Risk_DblMap[fip_d][fip_o] = risk\n",
    "    Exposure_Map = {}\n",
    "    for fip_d in Risk_DblMap:\n",
    "        unor_v = np.array(list(Risk_DblMap[fip_d].values()))\n",
    "        Sigma = np.sum(unor_v)\n",
    "        unor_v[unor_v == 0] = 10**-10\n",
    "        v = unor_v/Sigma\n",
    "        v = unor_v\n",
    "        logv = np.log(v)\n",
    "        entropy =  np.sum(v * logv)\n",
    "        Exposure_Map[fip_d] = entropy\n",
    "    return Exposure_Map"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 166,
   "metadata": {},
   "outputs": [],
   "source": [
    "def saveInlfowExposureToCSV(list_Risk_DblMap, path):\n",
    "    Exposure_Map1 = getInlfowExposure(list_Risk_DblMap[0])\n",
    "    Exposure_Map2 = getInlfowExposure(list_Risk_DblMap[1])\n",
    "    Exposure_Map3 = getInlfowExposure(list_Risk_DblMap[2])\n",
    "    Exposure_Map4 = getInlfowExposure(list_Risk_DblMap[3])\n",
    "    \n",
    "    exposure_df = pd.DataFrame(columns = ('FIPS', 'inflow_exposure1', 'inflow_exposure2', 'inflow_exposure3', 'inflow_exposure4'))\n",
    "    \n",
    "    for i, fip_d in enumerate(Exposure_Map1): \n",
    "        FIPS = str(fip_d)\n",
    "        exposure1 = Exposure_Map1[fip_d]\n",
    "        exposure2 = Exposure_Map2[fip_d]\n",
    "        exposure3 = Exposure_Map3[fip_d]\n",
    "        exposure4 = Exposure_Map4.get(fip_d, 0)\n",
    "        \n",
    "        new_row = pd.DataFrame({'FIPS':[FIPS], \n",
    "                                'inflow_exposure1':[exposure1], 'inflow_exposure2':[exposure2],\n",
    "                                'inflow_exposure3':[exposure3], 'inflow_exposure4':[exposure4]})\n",
    "        exposure_df = exposure_df.append(new_row)\n",
    "    \n",
    "    exposure_df.to_csv(path)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 167,
   "metadata": {},
   "outputs": [],
   "source": [
    "saveInlfowExposureToCSV(list_unor_Risk_DblMap, 'Streetlight_Data/clean_data/unnorm/unor_inflow_exposure_df.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
